{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob, time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = '../enron1/'\n",
    "HAM_FOLDER = 'ham/'\n",
    "SPAM_FOLDER = 'spam/'\n",
    "\n",
    "HAM_LIST = glob.glob(FOLDER + HAM_FOLDER + '*.txt')\n",
    "SPAM_LIST = glob.glob(FOLDER + SPAM_FOLDER + '*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \n",
    "    HAM = 0\n",
    "    SPAM = 1\n",
    "\n",
    "    def __init__(self, ham_list, spam_list, f_train = None):\n",
    "        self.ham_list = ham_list # file list of ham emails\n",
    "        self.spam_list = spam_list # file list of spam emails\n",
    "\n",
    "        self.N_HAM = np.size(ham_list) # number of total ham emails\n",
    "        self.N_SPAM = np.size(spam_list) # number of total spam emails\n",
    "        self.N = np.asarray([self.N_HAM, self.N_SPAM])\n",
    "        self.label = np.asarray([self.HAM]* self.N_HAM + [self.SPAM]* self.N_SPAM)\n",
    "        \n",
    "        # container for vocabulary list\n",
    "        self.vocab = None\n",
    "        self.nvocab = 0\n",
    "        \n",
    "        self.f_train = 0.8 if not f_train else f_train # fraction of train in total, default to be 0.8\n",
    "        # num of train docs in ham and spam folder\n",
    "        self.N_TRAIN = np.asarray([int(np.floor(self.N_HAM * self.f_train)), \n",
    "                                      int(np.floor(self.N_SPAM * self.f_train))]) \n",
    "        self.N_TEST = self.N - self.N_TRAIN\n",
    "        self.train_X, self.train_label, self.test_X, self.test_label = self.vectorize(self.f_train)\n",
    "        \n",
    "        self.result = None\n",
    "    \n",
    "    def vectorize(self, f_train = None):\n",
    "        start_time = time.time()\n",
    "        if f_train is not None:\n",
    "            self.f_train = f_train # else f_train = 0.8 by default\n",
    "            # [number of ham in train, number of spam in train]\n",
    "            self.N_TRAIN = np.asarray([int(np.floor(self.N_HAM * self.f_train)),\n",
    "                                          int(np.floor(self.N_SPAM * self.f_train))])\n",
    "            # [number of ham in test, number of spam in test]\n",
    "            self.N_TEST = self.N - self.N_TRAIN\n",
    "        print('vectorizing the emails...')\n",
    "        print('%s %% of emails are used for train...' % (self.f_train * 100))\n",
    "\n",
    "        # word stemming\n",
    "        # we are filtering out:\n",
    "        #    numbers, words shorter than 3 letters, words appeared less than 5 times in total,\n",
    "        #    words that appeared in 95% of the emails\n",
    "        pre = CountVectorizer(input = 'filename', decode_error = 'ignore', \n",
    "                              token_pattern = u'(?ui)\\\\b\\\\w*[a-z]+\\\\w{3,}\\\\b', max_df = 0.95, min_df = 5)\n",
    "        pre_X = pre.fit_transform(self.ham_list[:self.N_TRAIN[self.HAM]] + \n",
    "                                  self.spam_list[:self.N_TRAIN[self.SPAM]]).toarray()\n",
    "        \n",
    "        # get the vocabulary list from train data\n",
    "        prevocab = pre.get_feature_names()\n",
    "#         stemmer = EnglishStemmer()\n",
    "        stemmed = [EnglishStemmer().stem(w) for w in prevocab]\n",
    "        self.vocab = np.unique(stemmed)\n",
    "        self.nvocab = np.size(self.vocab)\n",
    "        \n",
    "        # train data vectorized with our vocabulary\n",
    "        train = CountVectorizer(input = 'filename', decode_error = 'ignore', vocabulary = self.vocab)\n",
    "        self.train_X = train.fit_transform(self.ham_list[:self.N_TRAIN[self.HAM]] + \n",
    "                                                 self.spam_list[:self.N_TRAIN[self.SPAM]]).toarray()\n",
    "\n",
    "        # test data vectorized with our vocabulary\n",
    "        test = CountVectorizer(input = 'filename', vocabulary = self.vocab, decode_error = 'ignore')\n",
    "        self.test_X = test.fit_transform(self.ham_list[-self.N_TEST[self.HAM]:] + \n",
    "                                               self.spam_list[-self.N_TEST[self.SPAM]:]).toarray()\n",
    "        \n",
    "        # create the label arrays\n",
    "        self.train_label = np.asarray([self.HAM] * self.N_TRAIN[self.HAM] + \n",
    "                                         [self.SPAM] * self.N_TRAIN[self.SPAM])\n",
    "        self.test_label = np.asarray([self.HAM] * self.N_TEST[self.HAM] + \n",
    "                                        [self.SPAM] * self.N_TEST[self.SPAM])\n",
    "        \n",
    "        print('vectorizing done! it took %.2f s' % (time.time() - start_time))\n",
    "        return self.train_X, self.train_label, self.test_X, self.test_label\n",
    "    \n",
    "    def get_train(self):\n",
    "        '''return the input matrix and label for train set\n",
    "            Output:\n",
    "                trainging_X, train_label'''\n",
    "        return self.train_X, self.train_label\n",
    "    \n",
    "    def get_test(self):\n",
    "        '''return the input matrix and label for test set\n",
    "            Output:\n",
    "                test_X, test_label'''\n",
    "        return self.test_X, self.test_label\n",
    "    \n",
    "    def split_ham_spam(self, X, label):\n",
    "        '''split X based on label HAM/SPAM'''\n",
    "        return X[np.where(label == self.HAM)], X[np.where(label == self.SPAM)]\n",
    "                \n",
    "    def accuracy(self, result = None):\n",
    "        if result is None:\n",
    "            if self.result is None:\n",
    "                print('no results recorded!')\n",
    "                return np.nan\n",
    "            else:\n",
    "                return np.mean(self.result == self.test_label) # num of correct predictions / total\n",
    "        else:\n",
    "            return np.mean(result == self.test_label) # num of correct predictions / total\n",
    "            \n",
    "\n",
    "    def naive_bayes(self, f_train = None):\n",
    "        if f_train is not None:\n",
    "            # re-vectorize the data\n",
    "            self.vectorize(f_train)\n",
    "        # we use the multinomial naive bayes model from \n",
    "        # https://web.stanford.edu/class/cs124/lec/naivebayes.pdf\n",
    "        def get_prior():\n",
    "            '''get the prior of for the Naive Bayes method which will be\n",
    "            [fraction of ham emails in train set, \n",
    "            fraction of spam emails in train set]'''\n",
    "            prior = self.N_TRAIN / self.N_TRAIN.sum()\n",
    "            return prior\n",
    "\n",
    "        def get_conditionals():\n",
    "            '''get the conditionals of for the Naive Bayes method with some smoothing'''\n",
    "            # split the traning data by label\n",
    "            train_ham, train_spam = self.split_ham_spam(self.train_X, self.train_label)\n",
    "\n",
    "            # conditionals with Laplace smoothing\n",
    "            con_ham = (train_ham.sum(axis = 0) + 1) / (train_ham.sum() + self.nvocab)\n",
    "            con_spam = (train_spam.sum(axis = 0) + 1) / (train_spam.sum() + self.nvocab)\n",
    "            conditionals = np.asarray([con_ham, con_spam])\n",
    "            return conditionals\n",
    "\n",
    "        print('cross validating...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        prior = get_prior()\n",
    "        conditionals = get_conditionals()\n",
    "        # start applying labels to our test data!\n",
    "        self.result = np.empty(self.N_TEST.sum()) # the results of our classifier\n",
    "        for i in np.arange(self.N_TEST.sum()):\n",
    "            # use log likelihood for easier calculation\n",
    "            loglike_ham = np.dot(np.log(conditionals[self.HAM]), self.test_X[i]) + np.log(prior[self.HAM])\n",
    "            loglike_spam = np.dot(np.log(conditionals[self.SPAM]), self.test_X[i]) + np.log(prior[self.SPAM])\n",
    "            self.result[i] = self.HAM if loglike_ham > loglike_spam else self.SPAM\n",
    "        print('test took %.2f s' % (time.time() - start_time))\n",
    "        return self.result\n",
    "\n",
    "    def nearest_neighbor(self, f_train = None):\n",
    "        if f_train != None:\n",
    "            # re-vectorize the data\n",
    "            self.vectorize(f_train)\n",
    "\n",
    "        print('running classifier...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        def calculate_l1_distance(train_row, test_row):\n",
    "            diff_row = np.subtract(train_row, test_row) # find element wise difference\n",
    "            diff_row = np.absolute(diff_row) # take absolute value of differences\n",
    "            distance = np.sum(diff_row) # sum the distances\n",
    "            return distance\n",
    "\n",
    "\n",
    "        def calculate_l2_distance(train_row, test_row):\n",
    "            diff_row = np.subtract(train_row, test_row)\n",
    "            diff_row = np.square(diff_row)\n",
    "            distance = np.sum(diff_row)\n",
    "            return np.sqrt(distance)\n",
    "\n",
    "\n",
    "        def calculate_linf_distance(train_row, test_row):\n",
    "            diff_row = np.subtract(train_row, test_row)\n",
    "            diff_row = np.absolute(diff_row)\n",
    "            return np.amax(diff_row)\n",
    "\n",
    "        predicted_label_l1 = np.empty(shape = (len(self.test_X), 1), dtype = int)\n",
    "        predicted_label_l2 = np.empty(shape = (len(self.test_X), 1), dtype = int)\n",
    "        predicted_label_linf = np.empty(shape = (len(self.test_X), 1), dtype = int)\n",
    "        for test_row, i in zip(self.test_X, range(len(self.test_X))):\n",
    "            row_distance_l1 = np.empty(shape = (len(self.train_X), 1), dtype = int)\n",
    "            row_distance_l2 = np.empty(shape = (len(self.train_X), 1), dtype = int)\n",
    "            row_distance_linf = np.empty(shape = (len(self.train_X), 1), dtype = int)\n",
    "            for train_row, j in zip(self.train_X, range(len(self.train_X))):\n",
    "                distance_l1 = calculate_l1_distance(train_row, test_row)\n",
    "                distance_l2 = calculate_l2_distance(train_row, test_row)\n",
    "                distance_linf = calculate_linf_distance(train_row, test_row)\n",
    "                row_distance_l1[j] = distance_l1 # array of distances for each test row\n",
    "                row_distance_l2[j] = distance_l2\n",
    "                row_distance_linf[j] = distance_linf\n",
    "                # print(\"test row:\", test_row, \"  | label: \", self.test_X_label[i])\n",
    "                # print(\"train row:\", train_row, \" | label: \", self.train_label[j])\n",
    "                # print(\"dist sum: \", distance)\n",
    "            min_dist_index_l1 = np.argmin(row_distance_l1) # min distance's index in array of distances\n",
    "            min_dist_index_l2 = np.argmin(row_distance_l2)\n",
    "            min_dist_index_linf = np.argmin(row_distance_linf)\n",
    "\n",
    "            predicted_label_l1[i] = self.train_label[min_dist_index_l1]\n",
    "            predicted_label_l2[i] = self.train_label[min_dist_index_l2]\n",
    "            predicted_label_linf[i] = self.train_label[min_dist_index_linf]\n",
    "            # print(\"-----------------------\")\n",
    "            # print(\"min dist: \", np.amin(row_distance))\n",
    "            # print(\"index of min: \", np.argmin(row_distance))\n",
    "            # print(\"predicted label: \", predicted_label[i])\n",
    "            # print(\"-----------------------\\n\")\n",
    "            self.result = [predicted_label_l1.flatten(), \n",
    "                           predicted_label_l2.flatten(), \n",
    "                           predicted_label_linf.flatten()]\n",
    "        print('test took %.2f s' % (time.time() - start_time))\n",
    "        return self.result\n",
    "    \n",
    "    def decision_tree(self, f_train = None):\n",
    "        if f_train is not None:\n",
    "            # re-vectorize the data\n",
    "            self.vectorize(f_train)\n",
    "        \n",
    "        print('running classifier...')\n",
    "        start_time = time.time()\n",
    "       \n",
    "        class TreeNode:\n",
    "            def __init__(self, idx):\n",
    "                self.idx = idx\n",
    "#                 print('a new node at index %d' % (self.idx, ))\n",
    "                self.value = np.nan\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "                \n",
    "        class ListNode:\n",
    "            def __init__(self, idx):\n",
    "                self.idx = idx\n",
    "                self.prev = None\n",
    "                self.next = None\n",
    "                \n",
    "            def is_head(self):\n",
    "                return True if self.prev is None else False\n",
    "            \n",
    "            def is_tail(self):\n",
    "                return True if self.next is None else False\n",
    "\n",
    "        class List:\n",
    "            def __init__(self, idx_list):\n",
    "                # Nodes for List\n",
    "                self.idx_list = idx_list\n",
    "                self.head = ListNode(idx_list[0])\n",
    "                self.length = 1\n",
    "                self.construct()\n",
    "\n",
    "            def construct(self):\n",
    "                current_Node = self.head\n",
    "                for i in np.arange(1, np.size(self.idx_list)):\n",
    "#                     print('the %d node is constructed with index %d' % (i, self.idx_list[i]))\n",
    "                    current_Node.next = ListNode(self.idx_list[i])\n",
    "                    current_Node.next.prev = current_Node\n",
    "                    current_Node = current_Node.next\n",
    "                    self.length += 1\n",
    "\n",
    "            def pop(self, current_node): \n",
    "                if self.length < 1:\n",
    "                    print('the size of list is %d. there is no nodes to pop.' % self.length)\n",
    "                elif self.length > 1:\n",
    "                    if current_node.is_head():\n",
    "                        next_n = current_node.next\n",
    "                        self.head = next_n\n",
    "                        next_n.prev = None\n",
    "                    elif current_node.is_tail():\n",
    "                        last_n = current_node.prev\n",
    "                        last_n.next = None\n",
    "                    else:\n",
    "                        last_n = current_node.prev\n",
    "                        next_n = current_node.next\n",
    "                        last_n.next = next_n\n",
    "                        next_n.prev = last_n\n",
    "                else: # list size = 1, this node is the only item in the List\n",
    "                    current_node = None\n",
    "                self.length -= 1\n",
    "\n",
    "            def is_empty(self):\n",
    "                return True if self.length < 1 else False  \n",
    "\n",
    "        def build_tree(rows):\n",
    "            '''build decision tree with the train data\n",
    "            Parameters:\n",
    "                rows: 1darray, row numbers that goes into this node; for root node, this is \n",
    "                np.arange(total_row_number_of_train_data); each row can be seen as one data\n",
    "                point'''\n",
    "            # if the region contains less data points than LEAF_SIZE, make this into a leaf\n",
    "            if np.size(rows) < LEAF_SIZE: \n",
    "                # the leaf is a label HAM or SPAM, depending only on the label in the leaf \n",
    "                # if fraction of HAM is higher, leaf is HAM and vice versa\n",
    "                if (self.train_label[rows] == self.HAM).mean() > 0.5: \n",
    "                    return self.HAM\n",
    "                else:\n",
    "                    return self.SPAM\n",
    "            else:\n",
    "                # get the head of arg_list, which is the position of the word with highest\n",
    "                # frequency difference in HAM vs. SPAM\n",
    "                current_node = arg_list.head\n",
    "                # the column slice of train_X at position arg_list.head.idx, the size is\n",
    "                # the same as the total number of data points(total number of rows)\n",
    "                col = self.train_X[:, current_node.idx]\n",
    "                # get the data points and their corresponding labels\n",
    "                # the list of frequency for word[idx]\n",
    "                node_data, node_labels = col[rows], self.train_label[rows]\n",
    "                current_value = unc_min(node_data, node_labels) # find the lowest gini index value for split\n",
    "                l_rows = rows[np.where(node_data <= current_value)] # split by value\n",
    "                r_rows = rows[np.where(node_data > current_value)]\n",
    "                while np.size(l_rows) * np.size(r_rows) == 0:\n",
    "                    if current_node.is_tail():\n",
    "#                         print('reached tail')\n",
    "                        break\n",
    "                    else:\n",
    "                        current_node = current_node.next\n",
    "    #                     print('looking for spliting point at %d' % idx)\n",
    "                        col = self.train_X[:, current_node.idx] # the column turned into array\n",
    "                        node_data = col[rows]\n",
    "                        node_labels = self.train_label[rows]\n",
    "                        # find the lowest gini index value for split\n",
    "                        current_value = unc_min(node_data, node_labels)\n",
    "                        l_rows = rows[np.where(node_data <= current_value)] # split by value\n",
    "                        r_rows = rows[np.where(node_data > current_value)]\n",
    "\n",
    "                new_N = TreeNode(current_node.idx)\n",
    "                new_N.value = current_value\n",
    "                arg_list.pop(current_node)\n",
    "                if arg_list.is_empty():\n",
    "                    print('arg_list is empty')\n",
    "                    if np.size(l_rows) == 0:\n",
    "                        if (self.train_label[r_rows] == self.HAM).mean() > 0.5:\n",
    "                            new_N.left, new_N.right = self.SPAM, self.HAM\n",
    "                        else:\n",
    "                            new_N.left, new_N.right = self.HAM, self.SPAM\n",
    "                        return new_N\n",
    "                    else:\n",
    "                        if (self.train_label[l_rows] == self.HAM).mean() > 0.5:\n",
    "                            new_N.right, new_N.left = self.SPAM, self.HAM\n",
    "                        else:\n",
    "                            new_N.right, new_N.left = self.HAM, self.SPAM\n",
    "                        return new_N\n",
    "                else: # arg_list not empty\n",
    "                    if np.size(l_rows) == 0:\n",
    "                        new_N.left = self.SPAM if (self.train_label[r_rows] == self.HAM).mean() > 0.5 else self.HAM\n",
    "                        new_N.right = build_tree(r_rows)\n",
    "                        return new_N\n",
    "                    elif np.size(r_rows) == 0:\n",
    "                        new_N.right = self.SPAM if (self.train_label[l_rows] == self.HAM).mean() > 0.5 else self.HAM\n",
    "                        new_N.left = build_tree(l_rows)\n",
    "                        return new_N\n",
    "                    else:\n",
    "                        new_N.left = build_tree(l_rows)\n",
    "                        new_N.right = build_tree(r_rows)\n",
    "                        return new_N\n",
    "\n",
    "\n",
    "        def unc_min(data, label):\n",
    "            '''return the frequency value of minimum uncertainty given an array \n",
    "            of word freq and associated label\n",
    "            Parameters:\n",
    "            col: 1darray of word frequencies at data points\n",
    "            label: 1darray of labels corresponding to these data points'''\n",
    "#             f_max, f_min = np.min(data[np.nonzero(data)]), np.max(data[np.nonzero(data)])\n",
    "            f_max, f_min = np.max(data), np.min(data) # the largest and the smallest value/freq in column\n",
    "            # if the maximun equals the minimum -> all values are equal\n",
    "            # return that value to split\n",
    "            if f_max == f_min: \n",
    "                return f_max \n",
    "            else: # if not all elements are zero\n",
    "                split_value = np.linspace(f_min, f_max, num = SPLIT) # the values of diff split\n",
    "                unc = uncertainty(split_value, data, label) # list of gini idx at diff split\n",
    "                return split_value[unc.argmin()] # return the value for min uncertainty\n",
    "\n",
    "        # Calculate the uncertainty for a split dataset\n",
    "        def _uncertainty(cut, data, label):\n",
    "            # labels for left and right nodes\n",
    "            l_label, r_label = label[np.where(data <= cut)], label[np.where(data > cut)]\n",
    "            l_len, r_len = np.size(l_label), np.size(r_label)\n",
    "            \n",
    "            unc = 0.0        \n",
    "            if l_len > 0:\n",
    "                l_p_ham = (l_label == self.HAM).mean()\n",
    "                l_p_spam = (l_label == self.SPAM).mean()\n",
    "                unc += (1 - (l_p_ham**2 + l_p_spam**2)) * (l_len / (l_len + r_len))\n",
    "\n",
    "            if r_len > 0:\n",
    "                r_p_ham = (r_label == self.HAM).mean()\n",
    "                r_p_spam = (r_label == self.SPAM).mean()\n",
    "                unc += (1 - (r_p_ham**2 + r_p_spam**2)) * (r_len / (l_len + r_len))\n",
    "            return unc\n",
    "        # vectorize this function so that it can take ndarrays as the first argument\n",
    "        uncertainty = np.vectorize(_uncertainty, excluded = [1, 2])\n",
    "        \n",
    "        def classify(case, N):\n",
    "            if N == self.HAM:\n",
    "#                 print('this is HAM')\n",
    "                return self.HAM\n",
    "            elif N == self.SPAM:\n",
    "#                 print('this is SPAM')\n",
    "                return self.SPAM\n",
    "            else:\n",
    "                if case[N.idx] <= N.value:\n",
    "#                     print('at %d (%s) freq is smaller than %f' % (N.idx, self.vocab[N.idx], N.value))\n",
    "                    return classify(case, N.left)\n",
    "                else:\n",
    "#                     print('at %d (%s) freq is greater than %f' % (N.idx, self.vocab[N.idx], N.value))\n",
    "                    return classify(case, N.right)\n",
    "                \n",
    "        LEAF_SIZE = 80 # maximum points at a leaf\n",
    "        SPLIT = 20\n",
    "        DICT_SIZE = 500\n",
    "        \n",
    "        train_ham, train_spam = self.split_ham_spam(self.train_X, self.train_label)\n",
    "        hmean, smean = train_ham.mean(axis = 0), train_spam.mean(axis = 0)\n",
    "\n",
    "        freq_diff = abs(hmean - smean) # difference of word freq in HAM vs. SPAM for each word\n",
    "        arg_fdiff = np.flip(np.argsort(abs(hmean - smean))) # idx of word in descending order of freq diff\n",
    "        arg_list = List(arg_fdiff[:DICT_SIZE])\n",
    "#         print(self.N_TRAIN.sum(),self.train_X.shape[0], self.nvocab)\n",
    "        # use all data points(rows) to build the tree\n",
    "        root = build_tree(np.arange(self.N_TRAIN.sum()))\n",
    "        self.result = np.empty(self.N_TEST.sum())\n",
    "        print('test...')\n",
    "        for i in np.arange(np.size(self.test_label)):\n",
    "            self.result[i] = classify(self.test_X[i], root)\n",
    "            \n",
    "        print('test took %.2f s' % (time.time() - start_time))\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing the emails...\n",
      "80.0 % of emails are used for train...\n",
      "vectorizing done! it took 1.03 s\n",
      "running classifier...\n",
      "arg_list is empty\n",
      "the size of list is 0. there is no nodes to pop.\n",
      "arg_list is empty\n",
      "the size of list is -1. there is no nodes to pop.\n",
      "arg_list is empty\n",
      "the size of list is -2. there is no nodes to pop.\n",
      "arg_list is empty\n",
      "the size of list is -3. there is no nodes to pop.\n",
      "arg_list is empty\n",
      "the size of list is -4. there is no nodes to pop.\n",
      "arg_list is empty\n",
      "the size of list is -5. there is no nodes to pop.\n",
      "arg_list is empty\n",
      "the size of list is -6. there is no nodes to pop.\n",
      "arg_list is empty\n",
      "the size of list is -7. there is no nodes to pop.\n",
      "arg_list is empty\n",
      "test...\n",
      "test took 0.96 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8415458937198068"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Classifier(HAM_LIST, SPAM_LIST, 0.8)\n",
    "result = test.decision_tree()\n",
    "test.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Classifier(HAM_LIST, SPAM_LIST, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test.naive_bayes(0.9)\n",
    "test.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.accuracy(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = List(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list.construct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(10, size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 3, 2, 3, 9, 7, 7, 4, 2, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a == 7).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = m.next\n",
    "d = c.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = test_list.head\n",
    "m = n.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list.pop(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = test_list.head\n",
    "while n is not None:\n",
    "    print(n.idx)\n",
    "    n = n.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
